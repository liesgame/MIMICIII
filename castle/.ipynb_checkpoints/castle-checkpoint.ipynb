{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0cd38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\liesgame\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 用于控制Python中小数的显示精度。\n",
    "#1.precision：控制输出结果的精度(即小数点后的位数)，默认值为8\n",
    "#2.threshold：当数组元素总数过大时，设置显示的数字位数，其余用省略号代替(当数组元素总数大于设置值，控制输出值得个数为6个，当数组元素小于或者等于设置值得时候，全部显示)，当设置值为sys.maxsize(需要导入sys库)，则会输出所有元素\n",
    "#3.linewidth：每行字符的数目，其余的数值会换到下一行\n",
    "#4.suppress：小数是否需要以科学计数法的形式输出\n",
    "#5.formatter：自定义输出规则\n",
    "###\n",
    "np.set_printoptions(suppress=True)\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "# 禁用TensorFlow 2.x行为。\n",
    "tf.disable_v2_behavior() \n",
    "from sklearn.metrics import mean_squared_error\n",
    "# this allows wider numpy viewing for matrices\n",
    "np.set_printoptions(linewidth=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96fc6a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0144f8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\liesgame\\AppData\\Local\\Temp\\ipykernel_10556\\337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CASTLE(object):\n",
    "    def __init__(self, num_train, lr  = None, batch_size = 32, num_inputs = 1, num_outputs = 1,\n",
    "                 w_threshold = 0.3, n_hidden = 32, hidden_layers = 2, ckpt_file = 'tmp.ckpt',\n",
    "                 standardize = True,  reg_lambda=None, reg_beta=None, DAG_min = 0.5):\n",
    "        \n",
    "        self.w_threshold = w_threshold\n",
    "        self.DAG_min = DAG_min\n",
    "        if lr is None:\n",
    "            self.learning_rate = 0.001\n",
    "        else:\n",
    "            self.learning_rate = lr\n",
    "        # 正则化系数 R DAG\n",
    "        if reg_lambda is None:\n",
    "            self.reg_lambda = 1.\n",
    "        else:\n",
    "            self.reg_lambda = reg_lambda\n",
    "        # R DAG 中 l1 norm of W 的 系数\n",
    "        if reg_beta is None:\n",
    "            self.reg_beta = 1\n",
    "        else:\n",
    "            self.reg_beta = reg_beta\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_inputs = num_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "        # num_input 是数据列的数量，也就是target+feature的 数量\n",
    "        # 多维数据，行不确定， 列 num_input\n",
    "        self.X = tf.placeholder(\"float\", [None, self.num_inputs])\n",
    "        #  n X 1 \n",
    "        self.y = tf.placeholder(\"float\", [None, 1])\n",
    "        self.rho =  tf.placeholder(\"float\",[1,1])\n",
    "        self.alpha =  tf.placeholder(\"float\",[1,1])\n",
    "        self.keep_prob = tf.placeholder(\"float\")\n",
    "        self.Lambda = tf.placeholder(\"float\")\n",
    "        self.noise = tf.placeholder(\"float\")\n",
    "        self.is_train = tf.placeholder(tf.bool, name=\"is_train\")\n",
    "\n",
    "        self.count = 0\n",
    "        self.max_steps = 200\n",
    "        self.saves = 50 \n",
    "        self.patience = 30\n",
    "        self.metric = mean_squared_error\n",
    "\n",
    "        \n",
    "        # One-hot vector indicating which nodes are trained\n",
    "        self.sample =tf.placeholder(tf.int32, [self.num_inputs])\n",
    "        \n",
    "        # Store layers weight & bias\n",
    "        seed = 1\n",
    "        self.weights = {}\n",
    "        # 偏差\n",
    "        self.biases = {}\n",
    "        \n",
    "        # Create the input and output weight matrix for each feature\n",
    "        # eg: 10 X 32\n",
    "        for i in range(self.num_inputs):\n",
    "            self.weights['w_h0_'+str(i)] = tf.Variable(tf.random_normal([self.num_inputs, self.n_hidden], seed = seed)*0.01)\n",
    "            self.weights['out_'+str(i)] = tf.Variable(tf.random_normal([self.n_hidden, self.num_outputs], seed = seed))\n",
    "            \n",
    "        for i in range(self.num_inputs):\n",
    "            self.biases['b_h0_'+str(i)] = tf.Variable(tf.random_normal([self.n_hidden], seed = seed)*0.01)\n",
    "            self.biases['out_'+str(i)] = tf.Variable(tf.random_normal([self.num_outputs], seed = seed))\n",
    "        \n",
    "        \n",
    "        # The first and second layers are shared\n",
    "        # 为什么要共享？\n",
    "        self.weights.update({\n",
    "            'w_h1': tf.Variable(tf.random_normal([self.n_hidden, self.n_hidden]))\n",
    "        })\n",
    "        \n",
    "        \n",
    "        self.biases.update({\n",
    "            'b_h1': tf.Variable(tf.random_normal([self.n_hidden]))\n",
    "        })\n",
    "        \n",
    "            \n",
    "        self.hidden_h0 = {}\n",
    "        self.hidden_h1 = {}\n",
    "        self.layer_1 = {}\n",
    "        self.layer_1_dropout = {}\n",
    "        self.out_layer = {}\n",
    "       \n",
    "        self.Out_0 = []\n",
    "        \n",
    "        # Mask removes the feature i from the network that is tasked to construct feature i\n",
    "        self.mask = {}\n",
    "        self.activation = tf.nn.relu\n",
    "            \n",
    "        for i in range(self.num_inputs):\n",
    "            indices = [i]*self.n_hidden\n",
    "            # eg. mask 10 X 32, 每一行有一个空的，features X hidden\n",
    "            self.mask[str(i)] = tf.transpose(tf.one_hot(indices, depth=self.num_inputs, on_value=0.0, off_value=1.0, axis=-1))\n",
    "            # 每次把i, 的属性设置为0\n",
    "            self.weights['w_h0_'+str(i)] = self.weights['w_h0_'+str(i)]*self.mask[str(i)] \n",
    "            self.hidden_h0['nn_'+str(i)] = self.activation(tf.add(tf.matmul(self.X, self.weights['w_h0_'+str(i)]), self.biases['b_h0_'+str(i)]))\n",
    "            self.hidden_h1['nn_'+str(i)] = self.activation(tf.add(tf.matmul(self.hidden_h0['nn_'+str(i)], self.weights['w_h1']), self.biases['b_h1']))\n",
    "            self.out_layer['nn_'+str(i)] = tf.matmul(self.hidden_h1['nn_'+str(i)], self.weights['out_'+str(i)]) + self.biases['out_'+str(i)]\n",
    "            # hidden X features\n",
    "            self.Out_0.append(self.out_layer['nn_'+str(i)])\n",
    "        \n",
    "        # Concatenate all the constructed features\n",
    "        self.Out = tf.concat(self.Out_0,axis=1)\n",
    "        # axis = 1, 对列进行相加\n",
    "        self.optimizer_subset = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # self.supervised_loss -》 predict\n",
    "        self.supervised_loss = tf.reduce_mean(tf.reduce_sum(tf.square(self.out_layer['nn_0'] - self.y),axis=1),axis=0)\n",
    "        self.regularization_loss = 0\n",
    "\n",
    "        self.W_0 = []\n",
    "        for i in range(self.num_inputs):\n",
    "            # 根号下平方和，来表示权重\n",
    "            self.W_0.append(tf.math.sqrt(tf.reduce_sum(tf.square(self.weights['w_h0_'+str(i)]),axis=1,keepdims=True)))\n",
    "        \n",
    "        # W features x features, \n",
    "        self.W = tf.concat(self.W_0,axis=1)\n",
    "               \n",
    "        #truncated power series\n",
    "        d = tf.cast(self.X.shape[1], tf.float32)\n",
    "        coff = 1.0 \n",
    "        Z = tf.multiply(self.W,self.W)\n",
    "       \n",
    "        dag_l = tf.cast(d, tf.float32) \n",
    "       \n",
    "        Z_in = tf.eye(d)\n",
    "        for i in range(1,10):\n",
    "           \n",
    "            Z_in = tf.matmul(Z_in, Z)\n",
    "           \n",
    "            dag_l += 1./coff * tf.linalg.trace(Z_in)\n",
    "            coff = coff * (i+1)\n",
    "        \n",
    "        self.h = dag_l - tf.cast(d, tf.float32)\n",
    "\n",
    "        # Residuals\n",
    "        self.R = self.X - self.Out \n",
    "        # Average reconstruction loss\n",
    "        self.average_loss = 0.5 / num_train * tf.reduce_sum(tf.square(self.R))\n",
    "\n",
    "\n",
    "        #group lasso\n",
    "        L1_loss = 0.0\n",
    "        for i in range(self.num_inputs):\n",
    "            w_1 = tf.slice(self.weights['w_h0_'+str(i)],[0,0],[i,-1])\n",
    "            w_2 = tf.slice(self.weights['w_h0_'+str(i)],[i+1,0],[-1,-1])\n",
    "            L1_loss += tf.reduce_sum(tf.norm(w_1,axis=1))+tf.reduce_sum(tf.norm(w_2,axis=1))\n",
    "        \n",
    "        # Divide the residual into untrain and train subset\n",
    "        # subset_R represent the value with 1 in sample\n",
    "        _, subset_R = tf.dynamic_partition(tf.transpose(self.R), partitions=self.sample, num_partitions=2)\n",
    "        subset_R = tf.transpose(subset_R)\n",
    "\n",
    "        #Combine all the loss\n",
    "        # features / the number of sample * sum of square residual\n",
    "        self.mse_loss_subset = tf.cast(self.num_inputs, tf.float32)/ tf.cast(tf.reduce_sum(self.sample), tf.float32)* tf.reduce_sum(tf.square(subset_R))\n",
    "        # ？ 按照公式 h 还得 - 1\n",
    "        #  self.mse_loss_subset -》 LW\n",
    "        #  L1_loss -> Vw\n",
    "        # self.alpha * self.h ? 这个没有对应的\n",
    "        self.regularization_loss_subset =  self.mse_loss_subset +  self.reg_beta * L1_loss +  0.5 * self.rho * self.h * self.h + self.alpha * self.h\n",
    "            \n",
    "        #Add in supervised loss\n",
    "        # ? self.Lambda 为什么放supervised_loss, 不应该在RADG?\n",
    "        self.regularization_loss_subset +=  self.Lambda *self.rho* self.supervised_loss\n",
    "        \n",
    "        # 最小化 loss dag function\n",
    "        self.loss_op_dag = self.optimizer_subset.minimize(self.regularization_loss_subset)\n",
    "\n",
    "        # 最小化 loss without dag function\n",
    "        self.loss_op_supervised = self.optimizer_subset.minimize(self.supervised_loss + self.regularization_loss)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())     \n",
    "        self.saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "        self.tmp = ckpt_file\n",
    "        \n",
    "    def __del__(self):\n",
    "        # 重置图，v1中， v2已经放弃\n",
    "        tf.reset_default_graph()\n",
    "        print(\"Destructor Called... Cleaning up\")\n",
    "        self.sess.close()\n",
    "        del self.sess\n",
    "        \n",
    "    def gaussian_noise_layer(self, input_layer, std):\n",
    "        noise = tf.random_normal(shape=tf.shape(input_layer), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "        return input_layer + noise\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y,num_nodes, X_val, y_val, X_test, y_test):         \n",
    "        \n",
    "        from random import sample \n",
    "        rho_i = np.array([[1.0]])\n",
    "        alpha_i = np.array([[1.0]])\n",
    "        \n",
    "        best = 1e9\n",
    "        best_value = 1e9\n",
    "        for step in range(1, self.max_steps):\n",
    "            h_value, loss = self.sess.run([self.h, self.supervised_loss], feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:rho_i, self.alpha:alpha_i, self.is_train : True, self.noise:0})\n",
    "            print(\"Step \" + str(step) + \", Loss= \" + \"{:.4f}\".format(loss),\" h_value:\", h_value) \n",
    "\n",
    "                \n",
    "            for step1 in range(1, (X.shape[0] // self.batch_size) + 1):\n",
    "\n",
    "               \n",
    "                idxs = random.sample(range(X.shape[0]), self.batch_size)\n",
    "                batch_x = X[idxs]\n",
    "                batch_y = np.expand_dims(batch_x[:,0], -1)\n",
    "                one_hot_sample = [0]*self.num_inputs\n",
    "                subset_ = sample(range(self.num_inputs),num_nodes) \n",
    "                for j in subset_:\n",
    "                    one_hot_sample[j] = 1\n",
    "                self.sess.run(self.loss_op_dag, feed_dict={self.X: batch_x, self.y: batch_y, self.sample:one_hot_sample,\n",
    "                                                              self.keep_prob : 1, self.rho:rho_i, self.alpha:alpha_i, self.Lambda : self.reg_lambda, self.is_train : True, self.noise : 0})\n",
    "\n",
    "            val_loss = self.val_loss(X_val, y_val)\n",
    "            if val_loss < best_value:\n",
    "                best_value = val_loss\n",
    "            h_value, loss = self.sess.run([self.h, self.supervised_loss], feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:rho_i, self.alpha:alpha_i, self.is_train : True, self.noise:0})\n",
    "            if step >= self.saves:\n",
    "                try:\n",
    "                    if val_loss < best:\n",
    "                        best = val_loss \n",
    "                        self.saver.save(self.sess, self.tmp)\n",
    "                        print(\"Saving model\")\n",
    "                        self.count = 0\n",
    "                    else:\n",
    "                        # when find model > best 意味着 模型开始走下坡路     \n",
    "                        self.count += 1\n",
    "                except:\n",
    "                    print(\"Error caught in calculation\")      \n",
    "            if self.count > self.patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        self.saver.restore(self.sess, self.tmp)\n",
    "        W_est = self.sess.run(self.W, feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:rho_i, self.alpha:alpha_i, self.is_train : True, self.noise:0})\n",
    "        W_est[np.abs(W_est) < self.w_threshold] = 0\n",
    "\n",
    "   \n",
    "    def val_loss(self, X, y):\n",
    "        if len(y.shape) < 2:\n",
    "            y = np.expand_dims(y, -1)\n",
    "        from random import sample \n",
    "        one_hot_sample = [0]*self.num_inputs\n",
    "        \n",
    "        # use all values for validation\n",
    "        subset_ = sample(range(self.num_inputs),self.num_inputs) \n",
    "        for j in subset_:\n",
    "            one_hot_sample[j] = 1\n",
    "        \n",
    "#         return self.sess.run(self.supervised_loss, feed_dict={self.X: X, self.y: y, self.sample:one_hot_sample, self.keep_prob : 1, self.rho:np.array([[1.0]]), \n",
    "#                                                               self.alpha:np.array([[0.0]]), self.Lambda : self.reg_lambda, self.is_train : False, self.noise:0})\n",
    "\n",
    "        return self.sess.run(self.supervised_loss, feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:np.array([[1.0]]), \n",
    "                                                              self.alpha:np.array([[0.0]]), self.Lambda : self.reg_lambda, self.is_train : False, self.noise:0})\n",
    "        \n",
    "    def pred(self, X):\n",
    "        return self.sess.run(self.out_layer['nn_0'], feed_dict={self.X: X, self.keep_prob:1, self.is_train : False, self.noise:0})\n",
    "        \n",
    "    def get_weights(self, X, y):\n",
    "        return self.sess.run(self.W, feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:np.array([[1.0]]), self.alpha:np.array([[0.0]]), self.is_train : False, self.noise:0})\n",
    "    \n",
    "    def pred_W(self, X, y):\n",
    "        W_est = self.sess.run(self.W, feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:np.array([[1.0]]), self.alpha:np.array([[0.0]]), self.is_train : False, self.noise:0})\n",
    "        return np.round_(W_est,decimals=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c853a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
