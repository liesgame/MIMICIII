{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6864732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\liesgame\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 用于控制Python中小数的显示精度。\n",
    "#1.precision：控制输出结果的精度(即小数点后的位数)，默认值为8\n",
    "#2.threshold：当数组元素总数过大时，设置显示的数字位数，其余用省略号代替(当数组元素总数大于设置值，控制输出值得个数为6个，当数组元素小于或者等于设置值得时候，全部显示)，当设置值为sys.maxsize(需要导入sys库)，则会输出所有元素\n",
    "#3.linewidth：每行字符的数目，其余的数值会换到下一行\n",
    "#4.suppress：小数是否需要以科学计数法的形式输出\n",
    "#5.formatter：自定义输出规则\n",
    "###\n",
    "np.set_printoptions(suppress=True)\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "# 禁用TensorFlow 2.x行为。\n",
    "tf.disable_v2_behavior() \n",
    "from sklearn.metrics import mean_squared_error\n",
    "# this allows wider numpy viewing for matrices\n",
    "np.set_printoptions(linewidth=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598d4336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a92cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\liesgame\\AppData\\Local\\Temp\\ipykernel_11688\\337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d03af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CASTLE(object):\n",
    "    def __init__(self, num_train, lr  = None, batch_size = 32, num_inputs = 1, num_outputs = 1,\n",
    "                 w_threshold = 0.3, n_hidden = 32, hidden_layers = 2, ckpt_file = 'tmp.ckpt',\n",
    "                 standardize = True,  reg_lambda=None, reg_beta=None, DAG_min = 0.5):\n",
    "        \n",
    "        self.w_threshold = w_threshold\n",
    "        self.DAG_min = DAG_min\n",
    "        if lr is None:\n",
    "            self.learning_rate = 0.001\n",
    "        else:\n",
    "            self.learning_rate = lr\n",
    "        # 正则化系数 R DAG\n",
    "        if reg_lambda is None:\n",
    "            self.reg_lambda = 1.\n",
    "        else:\n",
    "            self.reg_lambda = reg_lambda\n",
    "        # R DAG 中 l1 norm of W 的 系数\n",
    "        if reg_beta is None:\n",
    "            self.reg_beta = 1\n",
    "        else:\n",
    "            self.reg_beta = reg_beta\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_inputs = num_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "        # num_input 是数据列的数量，也就是target+feature的 数量\n",
    "        # 多维数据，行不确定， 列 num_input\n",
    "        self.X = tf.placeholder(\"float\", [None, self.num_inputs])\n",
    "        #  n X 1 \n",
    "        self.y = tf.placeholder(\"float\", [None, 1])\n",
    "        self.rho =  tf.placeholder(\"float\",[1,1])\n",
    "        self.alpha =  tf.placeholder(\"float\",[1,1])\n",
    "        self.keep_prob = tf.placeholder(\"float\")\n",
    "        self.Lambda = tf.placeholder(\"float\")\n",
    "        self.noise = tf.placeholder(\"float\")\n",
    "        self.is_train = tf.placeholder(tf.bool, name=\"is_train\")\n",
    "\n",
    "        self.count = 0\n",
    "        self.max_steps = 200\n",
    "        self.saves = 50 \n",
    "        self.patience = 30\n",
    "        self.metric = mean_squared_error\n",
    "\n",
    "        \n",
    "        # One-hot vector indicating which nodes are trained\n",
    "        self.sample =tf.placeholder(tf.int32, [self.num_inputs])\n",
    "        \n",
    "        # Store layers weight & bias\n",
    "        seed = 1\n",
    "        self.weights = {}\n",
    "        # 偏差\n",
    "        self.biases = {}\n",
    "        \n",
    "        # Create the input and output weight matrix for each feature\n",
    "        # eg: 10 X 32\n",
    "        for i in range(self.num_inputs):\n",
    "            self.weights['w_h0_'+str(i)] = tf.Variable(tf.random_normal([self.num_inputs, self.n_hidden], seed = seed)*0.01)\n",
    "            self.weights['out_'+str(i)] = tf.Variable(tf.random_normal([self.n_hidden, self.num_outputs], seed = seed))\n",
    "            \n",
    "        for i in range(self.num_inputs):\n",
    "            self.biases['b_h0_'+str(i)] = tf.Variable(tf.random_normal([self.n_hidden], seed = seed)*0.01)\n",
    "            self.biases['out_'+str(i)] = tf.Variable(tf.random_normal([self.num_outputs], seed = seed))\n",
    "        \n",
    "        \n",
    "        # The first and second layers are shared\n",
    "        # 为什么要共享？\n",
    "        self.weights.update({\n",
    "            'w_h1': tf.Variable(tf.random_normal([self.n_hidden, self.n_hidden]))\n",
    "        })\n",
    "        \n",
    "        \n",
    "        self.biases.update({\n",
    "            'b_h1': tf.Variable(tf.random_normal([self.n_hidden]))\n",
    "        })\n",
    "        \n",
    "            \n",
    "        self.hidden_h0 = {}\n",
    "        self.hidden_h1 = {}\n",
    "        self.layer_1 = {}\n",
    "        self.layer_1_dropout = {}\n",
    "        self.out_layer = {}\n",
    "       \n",
    "        self.Out_0 = []\n",
    "        \n",
    "        # Mask removes the feature i from the network that is tasked to construct feature i\n",
    "        self.mask = {}\n",
    "        self.activation = tf.nn.relu\n",
    "            \n",
    "        for i in range(self.num_inputs):\n",
    "            indices = [i]*self.n_hidden\n",
    "            # eg. mask 10 X 32, 每一行有一个空的，features X hidden\n",
    "            self.mask[str(i)] = tf.transpose(tf.one_hot(indices, depth=self.num_inputs, on_value=0.0, off_value=1.0, axis=-1))\n",
    "            # 每次把i, 的属性设置为0\n",
    "            self.weights['w_h0_'+str(i)] = self.weights['w_h0_'+str(i)]*self.mask[str(i)] \n",
    "            self.hidden_h0['nn_'+str(i)] = self.activation(tf.add(tf.matmul(self.X, self.weights['w_h0_'+str(i)]), self.biases['b_h0_'+str(i)]))\n",
    "            self.hidden_h1['nn_'+str(i)] = self.activation(tf.add(tf.matmul(self.hidden_h0['nn_'+str(i)], self.weights['w_h1']), self.biases['b_h1']))\n",
    "            self.out_layer['nn_'+str(i)] = tf.matmul(self.hidden_h1['nn_'+str(i)], self.weights['out_'+str(i)]) + self.biases['out_'+str(i)]\n",
    "            # hidden X features\n",
    "            self.Out_0.append(self.out_layer['nn_'+str(i)])\n",
    "        \n",
    "        # Concatenate all the constructed features\n",
    "        self.Out = tf.concat(self.Out_0,axis=1)\n",
    "        # axis = 1, 对列进行相加\n",
    "        self.optimizer_subset = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # self.supervised_loss -》 predict\n",
    "        self.supervised_loss = tf.reduce_mean(tf.reduce_sum(tf.square(self.out_layer['nn_0'] - self.y),axis=1),axis=0)\n",
    "        self.regularization_loss = 0\n",
    "\n",
    "        self.W_0 = []\n",
    "        for i in range(self.num_inputs):\n",
    "            # 根号下平方和，来表示权重\n",
    "            self.W_0.append(tf.math.sqrt(tf.reduce_sum(tf.square(self.weights['w_h0_'+str(i)]),axis=1,keepdims=True)))\n",
    "        \n",
    "        # W features x features, \n",
    "        self.W = tf.concat(self.W_0,axis=1)\n",
    "               \n",
    "        #truncated power series\n",
    "        d = tf.cast(self.X.shape[1], tf.float32)\n",
    "        coff = 1.0 \n",
    "        Z = tf.multiply(self.W,self.W)\n",
    "       \n",
    "        dag_l = tf.cast(d, tf.float32) \n",
    "       \n",
    "        Z_in = tf.eye(d)\n",
    "        for i in range(1,10):\n",
    "           \n",
    "            Z_in = tf.matmul(Z_in, Z)\n",
    "           \n",
    "            dag_l += 1./coff * tf.linalg.trace(Z_in)\n",
    "            coff = coff * (i+1)\n",
    "        \n",
    "        self.h = dag_l - tf.cast(d, tf.float32)\n",
    "\n",
    "        # Residuals\n",
    "        self.R = self.X - self.Out \n",
    "        # Average reconstruction loss\n",
    "        self.average_loss = 0.5 / num_train * tf.reduce_sum(tf.square(self.R))\n",
    "\n",
    "\n",
    "        #group lasso\n",
    "        L1_loss = 0.0\n",
    "        for i in range(self.num_inputs):\n",
    "            w_1 = tf.slice(self.weights['w_h0_'+str(i)],[0,0],[i,-1])\n",
    "            w_2 = tf.slice(self.weights['w_h0_'+str(i)],[i+1,0],[-1,-1])\n",
    "            L1_loss += tf.reduce_sum(tf.norm(w_1,axis=1))+tf.reduce_sum(tf.norm(w_2,axis=1))\n",
    "        \n",
    "        # Divide the residual into untrain and train subset\n",
    "        # subset_R represent the value with 1 in sample\n",
    "        _, subset_R = tf.dynamic_partition(tf.transpose(self.R), partitions=self.sample, num_partitions=2)\n",
    "        subset_R = tf.transpose(subset_R)\n",
    "\n",
    "        #Combine all the loss\n",
    "        # features / the number of sample * sum of square residual\n",
    "        self.mse_loss_subset = tf.cast(self.num_inputs, tf.float32)/ tf.cast(tf.reduce_sum(self.sample), tf.float32)* tf.reduce_sum(tf.square(subset_R))\n",
    "        # ？ 按照公式 h 还得 - 1\n",
    "        #  self.mse_loss_subset -》 LW\n",
    "        #  L1_loss -> Vw\n",
    "        # self.alpha * self.h ? 这个没有对应的\n",
    "        self.regularization_loss_subset =  self.mse_loss_subset +  self.reg_beta * L1_loss +  0.5 * self.rho * self.h * self.h + self.alpha * self.h\n",
    "            \n",
    "        #Add in supervised loss\n",
    "        # ? self.Lambda 为什么放supervised_loss, 不应该在RADG?\n",
    "        self.regularization_loss_subset +=  self.Lambda *self.rho* self.supervised_loss\n",
    "        \n",
    "        # 最小化 loss dag function\n",
    "        self.loss_op_dag = self.optimizer_subset.minimize(self.regularization_loss_subset)\n",
    "\n",
    "        # 最小化 loss without dag function\n",
    "        self.loss_op_supervised = self.optimizer_subset.minimize(self.supervised_loss + self.regularization_loss)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())     \n",
    "        self.saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "        self.tmp = ckpt_file\n",
    "        \n",
    "    def __del__(self):\n",
    "        # 重置图，v1中， v2已经放弃\n",
    "        tf.reset_default_graph()\n",
    "        print(\"Destructor Called... Cleaning up\")\n",
    "        self.sess.close()\n",
    "        del self.sess\n",
    "        \n",
    "    def gaussian_noise_layer(self, input_layer, std):\n",
    "        noise = tf.random_normal(shape=tf.shape(input_layer), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "        return input_layer + noise\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y,num_nodes, X_val, y_val, X_test, y_test):         \n",
    "        \n",
    "        from random import sample \n",
    "        rho_i = np.array([[1.0]])\n",
    "        alpha_i = np.array([[1.0]])\n",
    "        \n",
    "        best = 1e9\n",
    "        best_value = 1e9\n",
    "        for step in range(1, self.max_steps):\n",
    "            h_value, loss = self.sess.run([self.h, self.supervised_loss], feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:rho_i, self.alpha:alpha_i, self.is_train : True, self.noise:0})\n",
    "            print(\"Step \" + str(step) + \", Loss= \" + \"{:.4f}\".format(loss),\" h_value:\", h_value) \n",
    "                \n",
    "            for step1 in range(1, (X.shape[0] // self.batch_size) + 1):\n",
    "\n",
    "               \n",
    "                idxs = random.sample(range(X.shape[0]), self.batch_size)\n",
    "                batch_x = X[idxs]\n",
    "                batch_y = np.expand_dims(batch_x[:,0], -1)\n",
    "                one_hot_sample = [0]*self.num_inputs\n",
    "                subset_ = sample(range(self.num_inputs),num_nodes) \n",
    "                for j in subset_:\n",
    "                    one_hot_sample[j] = 1\n",
    "                self.sess.run(self.loss_op_supervised, feed_dict={self.X: batch_x, self.y: batch_y, self.sample:one_hot_sample,\n",
    "                                                              self.keep_prob : 1, self.rho:rho_i, self.alpha:alpha_i, self.Lambda : self.reg_lambda, self.is_train : True, self.noise : 0})\n",
    "\n",
    "            val_loss = self.val_loss(X_val, y_val)\n",
    "            if val_loss < best_value:\n",
    "                best_value = val_loss\n",
    "            h_value, loss = self.sess.run([self.h, self.supervised_loss], feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:rho_i, self.alpha:alpha_i, self.is_train : True, self.noise:0})\n",
    "            if step >= self.saves:\n",
    "                try:\n",
    "                    if val_loss < best:\n",
    "                        best = val_loss \n",
    "                        self.saver.save(self.sess, self.tmp)\n",
    "                        print(\"Saving model\")\n",
    "                        self.count = 0\n",
    "                    else:\n",
    "                        # when find model > best 意味着 模型开始走下坡路     \n",
    "                        self.count += 1\n",
    "                except:\n",
    "                    print(\"Error caught in calculation\")      \n",
    "            if self.count > self.patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        self.saver.restore(self.sess, self.tmp)\n",
    "        W_est = self.sess.run(self.W, feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:rho_i, self.alpha:alpha_i, self.is_train : True, self.noise:0})\n",
    "        W_est[np.abs(W_est) < self.w_threshold] = 0\n",
    "\n",
    "   \n",
    "    def val_loss(self, X, y):\n",
    "        if len(y.shape) < 2:\n",
    "            y = np.expand_dims(y, -1)\n",
    "        from random import sample \n",
    "        one_hot_sample = [0]*self.num_inputs\n",
    "        \n",
    "        # use all values for validation\n",
    "        subset_ = sample(range(self.num_inputs),self.num_inputs) \n",
    "        for j in subset_:\n",
    "            one_hot_sample[j] = 1\n",
    "        \n",
    "#         return self.sess.run(self.supervised_loss, feed_dict={self.X: X, self.y: y, self.sample:one_hot_sample, self.keep_prob : 1, self.rho:np.array([[1.0]]), \n",
    "#                                                               self.alpha:np.array([[0.0]]), self.Lambda : self.reg_lambda, self.is_train : False, self.noise:0})\n",
    "\n",
    "        return self.sess.run(self.supervised_loss, feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:np.array([[1.0]]), \n",
    "                                                              self.alpha:np.array([[0.0]]), self.Lambda : self.reg_lambda, self.is_train : False, self.noise:0})\n",
    "        \n",
    "    def pred(self, X):\n",
    "        return self.sess.run(self.out_layer['nn_0'], feed_dict={self.X: X, self.keep_prob:1, self.is_train : False, self.noise:0})\n",
    "        \n",
    "    def get_weights(self, X, y):\n",
    "        return self.sess.run(self.W, feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:np.array([[1.0]]), self.alpha:np.array([[0.0]]), self.is_train : False, self.noise:0})\n",
    "    \n",
    "    def pred_W(self, X, y):\n",
    "        W_est = self.sess.run(self.W, feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:np.array([[1.0]]), self.alpha:np.array([[0.0]]), self.is_train : False, self.noise:0})\n",
    "        return np.round_(W_est,decimals=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, num_train, lr  = None, batch_size = 32, num_inputs = 1, num_outputs = 1,\n",
    "                 w_threshold = 0.3, n_hidden = 32, hidden_layers = 2, ckpt_file = 'tmp.ckpt',\n",
    "                 standardize = True,  reg_lambda=None, reg_beta=None, DAG_min = 0.5):\n",
    "        \n",
    "        self.w_threshold = w_threshold\n",
    "        self.DAG_min = DAG_min\n",
    "        if lr is None:\n",
    "            self.learning_rate = 0.001\n",
    "        else:\n",
    "            self.learning_rate = lr\n",
    "        # 正则化系数 R DAG\n",
    "        if reg_lambda is None:\n",
    "            self.reg_lambda = 1.\n",
    "        else:\n",
    "            self.reg_lambda = reg_lambda\n",
    "        # R DAG 中 l1 norm of W 的 系数\n",
    "        if reg_beta is None:\n",
    "            self.reg_beta = 1\n",
    "        else:\n",
    "            self.reg_beta = reg_beta\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_inputs = num_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "        # num_input 是数据列的数量，也就是target+feature的 数量\n",
    "        # 多维数据，行不确定， 列 num_input\n",
    "        self.X = tf.placeholder(\"float\", [None, self.num_inputs])\n",
    "        #  n X 1 \n",
    "        self.y = tf.placeholder(\"float\", [None, 1])\n",
    "        self.rho =  tf.placeholder(\"float\",[1,1])\n",
    "        self.alpha =  tf.placeholder(\"float\",[1,1])\n",
    "        self.keep_prob = tf.placeholder(\"float\")\n",
    "        self.Lambda = tf.placeholder(\"float\")\n",
    "        self.noise = tf.placeholder(\"float\")\n",
    "        self.is_train = tf.placeholder(tf.bool, name=\"is_train\")\n",
    "\n",
    "        self.count = 0\n",
    "        self.max_steps = 200\n",
    "        self.saves = 50 \n",
    "        self.patience = 30\n",
    "        self.metric = mean_squared_error\n",
    "\n",
    "        \n",
    "        # One-hot vector indicating which nodes are trained\n",
    "        self.sample =tf.placeholder(tf.int32, [self.num_inputs])\n",
    "        \n",
    "        # Store layers weight & bias\n",
    "        seed = 1\n",
    "        self.weights = {}\n",
    "        # 偏差\n",
    "        self.biases = {}\n",
    "        \n",
    "        \n",
    "        # Create the input and output weight matrix for each feature\n",
    "        # eg: 10 X 32\n",
    "        self.weights['w_h0'] = tf.Variable(tf.random_normal([self.num_inputs, self.n_hidden], seed = seed)*0.01)\n",
    "        self.weights['out'] = tf.Variable(tf.random_normal([self.n_hidden, self.num_outputs], seed = seed))\n",
    "            \n",
    "        self.biases['b_h0'] = tf.Variable(tf.random_normal([self.n_hidden], seed = seed)*0.01)\n",
    "        self.biases['out'] = tf.Variable(tf.random_normal([self.num_outputs], seed = seed))\n",
    "        \n",
    "        \n",
    "        # The first and second layers are shared\n",
    "        # 为什么要共享？\n",
    "        self.weights.update({\n",
    "            'w_h1': tf.Variable(tf.random_normal([self.n_hidden, self.n_hidden]))\n",
    "        })\n",
    "        \n",
    "        \n",
    "        self.biases.update({\n",
    "            'b_h1': tf.Variable(tf.random_normal([self.n_hidden]))\n",
    "        })\n",
    "        \n",
    "            \n",
    "        self.hidden_h0 = {}\n",
    "        self.hidden_h1 = {}\n",
    "        self.layer_1 = {}\n",
    "        self.layer_1_dropout = {}\n",
    "        self.out_layer = {}\n",
    "       \n",
    "        self.Out_0 = []\n",
    "        \n",
    "        # Mask removes the feature i from the network that is tasked to construct feature i\n",
    "        self.mask = {}\n",
    "        self.activation = tf.nn.relu\n",
    "            \n",
    "        indices = [i]*self.n_hidden\n",
    "        # eg. mask 10 X 32, 每一行有一个空的，features X hidden\n",
    "        self.mask = tf.transpose(tf.one_hot(indices, depth=self.num_inputs, on_value=0.0, off_value=1.0, axis=-1))\n",
    "        # 每次把i, 的属性设置为0\n",
    "        self.weights['w_h0'] = self.weights['w_h0'+str(i)]*self.mask\n",
    "        self.hidden_h0['nn'] = self.activation(tf.add(tf.matmul(self.X, self.weights['w_h0']), self.biases['b_h0']))\n",
    "        self.hidden_h1['nn'] = self.activation(tf.add(tf.matmul(self.hidden_h0['nn'], self.weights['w_h1']), self.biases['b_h1']))\n",
    "        self.out_layer['nn'] = tf.matmul(self.hidden_h1['nn'], self.weights['out']) + self.biases['out']\n",
    "\n",
    "        self.optimizer_subset = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # self.supervised_loss -》 predict\n",
    "        self.supervised_loss = tf.reduce_mean(tf.reduce_sum(tf.square(self.out_layer['nn'] - self.y),axis=1),axis=0)\n",
    "        \n",
    "        \n",
    "        self.W = tf.math.sqrt(tf.reduce_sum(tf.square(self.weights['w_h0']),axis=1,keepdims=True))\n",
    "\n",
    "        # 最小化 loss without dag function\n",
    "        self.loss_op_supervised = self.optimizer_subset.minimize(self.supervised_loss)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())     \n",
    "        self.saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "        self.tmp = ckpt_file\n",
    "        \n",
    "    def __del__(self):\n",
    "        # 重置图，v1中， v2已经放弃\n",
    "        tf.reset_default_graph()\n",
    "        print(\"Destructor Called... Cleaning up\")\n",
    "        self.sess.close()\n",
    "        del self.sess\n",
    "        \n",
    "    def gaussian_noise_layer(self, input_layer, std):\n",
    "        noise = tf.random_normal(shape=tf.shape(input_layer), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "        return input_layer + noise\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y,num_nodes, X_val, y_val, X_test, y_test):         \n",
    "        \n",
    "        from random import sample \n",
    "        rho_i = np.array([[1.0]])\n",
    "        alpha_i = np.array([[1.0]])\n",
    "        \n",
    "        best = 1e9\n",
    "        best_value = 1e9\n",
    "        for step in range(1, self.max_steps):\n",
    "            h_value, loss = self.sess.run([self.h, self.supervised_loss], feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:rho_i, self.alpha:alpha_i, self.is_train : True, self.noise:0})\n",
    "            print(\"Step \" + str(step) + \", Loss= \" + \"{:.4f}\".format(loss),\" h_value:\", h_value) \n",
    "\n",
    "                \n",
    "            for step1 in range(1, (X.shape[0] // self.batch_size) + 1):\n",
    "\n",
    "               \n",
    "                idxs = random.sample(range(X.shape[0]), self.batch_size)\n",
    "                batch_x = X[idxs]\n",
    "                batch_y = np.expand_dims(batch_x[:,0], -1)\n",
    "                one_hot_sample = [0]*self.num_inputs\n",
    "                subset_ = sample(range(self.num_inputs),num_nodes) \n",
    "                for j in subset_:\n",
    "                    one_hot_sample[j] = 1\n",
    "                self.sess.run(self.loss_op_dag, feed_dict={self.X: batch_x, self.y: batch_y, self.sample:one_hot_sample,\n",
    "                                                              self.keep_prob : 1, self.rho:rho_i, self.alpha:alpha_i, self.Lambda : self.reg_lambda, self.is_train : True, self.noise : 0})\n",
    "\n",
    "            val_loss = self.val_loss(X_val, y_val)\n",
    "            if val_loss < best_value:\n",
    "                best_value = val_loss\n",
    "            h_value, loss = self.sess.run([self.h, self.supervised_loss], feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:rho_i, self.alpha:alpha_i, self.is_train : True, self.noise:0})\n",
    "            if step >= self.saves:\n",
    "                try:\n",
    "                    if val_loss < best:\n",
    "                        best = val_loss \n",
    "                        self.saver.save(self.sess, self.tmp)\n",
    "                        print(\"Saving model\")\n",
    "                        self.count = 0\n",
    "                    else:\n",
    "                        # when find model > best 意味着 模型开始走下坡路     \n",
    "                        self.count += 1\n",
    "                except:\n",
    "                    print(\"Error caught in calculation\")      \n",
    "            if self.count > self.patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        self.saver.restore(self.sess, self.tmp)\n",
    "        W_est = self.sess.run(self.W, feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:rho_i, self.alpha:alpha_i, self.is_train : True, self.noise:0})\n",
    "        W_est[np.abs(W_est) < self.w_threshold] = 0\n",
    "\n",
    "   \n",
    "    def val_loss(self, X, y):\n",
    "        if len(y.shape) < 2:\n",
    "            y = np.expand_dims(y, -1)\n",
    "        from random import sample \n",
    "        one_hot_sample = [0]*self.num_inputs\n",
    "        \n",
    "        # use all values for validation\n",
    "        subset_ = sample(range(self.num_inputs),self.num_inputs) \n",
    "        for j in subset_:\n",
    "            one_hot_sample[j] = 1\n",
    "        \n",
    "#         return self.sess.run(self.supervised_loss, feed_dict={self.X: X, self.y: y, self.sample:one_hot_sample, self.keep_prob : 1, self.rho:np.array([[1.0]]), \n",
    "#                                                               self.alpha:np.array([[0.0]]), self.Lambda : self.reg_lambda, self.is_train : False, self.noise:0})\n",
    "\n",
    "        return self.sess.run(self.supervised_loss, feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:np.array([[1.0]]), \n",
    "                                                              self.alpha:np.array([[0.0]]), self.Lambda : self.reg_lambda, self.is_train : False, self.noise:0})\n",
    "        \n",
    "    def pred(self, X):\n",
    "        return self.sess.run(self.out_layer['nn_0'], feed_dict={self.X: X, self.keep_prob:1, self.is_train : False, self.noise:0})\n",
    "        \n",
    "    def get_weights(self, X, y):\n",
    "        return self.sess.run(self.W, feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:np.array([[1.0]]), self.alpha:np.array([[0.0]]), self.is_train : False, self.noise:0})\n",
    "    \n",
    "    def pred_W(self, X, y):\n",
    "        W_est = self.sess.run(self.W, feed_dict={self.X: X, self.y: y, self.keep_prob : 1, self.rho:np.array([[1.0]]), self.alpha:np.array([[0.0]]), self.is_train : False, self.noise:0})\n",
    "        return np.round_(W_est,decimals=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499c2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import networkx as nx\n",
    "import random\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "#Disable TensorFlow 2 behaviour\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from CASTLE import CASTLE\n",
    "from utils import random_dag, gen_data_nonlinear\n",
    "from signal import signal, SIGINT\n",
    "from sys import exit\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "511882ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7437c03",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766634a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Image\n",
    "from scipy.stats import ttest_ind_from_stats, spearmanr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0860e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dag = False\n",
    "# 一共多少属性\n",
    "num_nodes = 10\n",
    "# 一个属性和多少属性相关\n",
    "branchf = 4\n",
    "# 抽样数量\n",
    "dataset_sz = 1000\n",
    "test_rate = 0.25\n",
    "csv='synth_nonlinear.csv'\n",
    "output_log= 'castle.log'\n",
    "n_folds= 10\n",
    "reg_lambda = 1\n",
    "reg_beta = 5\n",
    "gpu = ''\n",
    "ckpt_file = 'tmp.ckpt'\n",
    "extension = ''\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "w_threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4442d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a820e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = 'D:/data/MIMIC_Extract/samples.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3af856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = h5py.File(DATAFILE, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f76651a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['interventions', 'patients', 'vitalslabs']>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62961348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['_i_table', 'meta', 'table']>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients['patients'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "619387fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, [1], [ 6980516880000000000, -9223372036854775808,  6981187260000000000, -9223372036854775808,  6980516971000000000,  6980661991000000000, -9223372036854775808], [47.84504657,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  1.        ,  1.        ,  1.67847222], [b'FEVER,DEHYDRATION,FAILURE TO THRIVE', b'HOME WITH HOME IV PROVIDR', b'Private'], [36], [2], [0], [ 0,  1, 40,  0,  0,  0], 294638, 185777, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients['patients']['table'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ea95fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['hadm_id', 'icustay_id', 'index', 'subject_id']>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients['patients']['_i_table'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6360a975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['abounds', 'bounds', 'indices', 'indicesLR', 'mbounds', 'mranges', 'ranges', 'sorted', 'sortedLR', 'zbounds']>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients['patients']['_i_table']['hadm_id'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "999747db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"abounds\": shape (0,), type \"<i8\">"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients['patients']['_i_table']['hadm_id']['abounds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f79e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_str(mean, std):\n",
    "    return \"${0:.6f}\".format(round(mean,6)) + \" \\pm {0:.6f}$    \".format(round(std,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e08a19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if random_dag:\n",
    "    def swap_cols(df, a, b):\n",
    "        df = df.rename(columns = {a : 'temp'})\n",
    "        df = df.rename(columns = {b : a})\n",
    "        return df.rename(columns = {'temp' : b})\n",
    "    def swap_nodes(G, a, b):\n",
    "        newG = nx.relabel_nodes(G, {a : 'temp'})\n",
    "        newG = nx.relabel_nodes(newG, {b : a})\n",
    "        return nx.relabel_nodes(newG, {'temp' : b})\n",
    "\n",
    "    #Random DAG\n",
    "    num_edges = int(num_nodes*branchf)\n",
    "    G = random_dag(num_nodes, num_edges)\n",
    "\n",
    "    # 返回浮点数[0.3, 1), 这个是方差\n",
    "    noise = random.uniform(0.3, 1.0)\n",
    "    print(\"Setting noise to \", noise)\n",
    "\n",
    "    df = gen_data_nonlinear(G, SIZE = dataset_sz, var = noise).iloc[:dataset_sz]\n",
    "    df_test =  gen_data_nonlinear(G, SIZE = int(dataset_sz*test_rate), var = noise)\n",
    "\n",
    "    # 保证0 也就是 target 一定有逻辑父节点\n",
    "    for i in range(len(G.edges())):\n",
    "        if len(list(G.predecessors(i))) > 0:\n",
    "            df = swap_cols(df, str(0), str(i))\n",
    "            df_test = swap_cols(df_test, str(0), str(i))\n",
    "            G = swap_nodes(G, 0, i)\n",
    "            break      \n",
    "\n",
    "    #print(\"Number of parents of G\", len(list(G.predecessors(i))))\n",
    "    print(\"Edges = \", list(G.edges()))\n",
    "\n",
    "else:\n",
    "    '''\n",
    "    Toy DAG\n",
    "    The node '0' is the target in the Toy DAG\n",
    "    '''\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(10):\n",
    "        G.add_node(i)\n",
    "    G.add_edge(1,2)\n",
    "    G.add_edge(1,3)\n",
    "    G.add_edge(1,4)\n",
    "    G.add_edge(2,5)\n",
    "    G.add_edge(2,0)\n",
    "    G.add_edge(3,0)\n",
    "    G.add_edge(3,6)\n",
    "    G.add_edge(3,7)\n",
    "    G.add_edge(6,9)\n",
    "    G.add_edge(0,8)\n",
    "    G.add_edge(0,9)\n",
    "\n",
    "    if csv:\n",
    "        df = pd.read_csv(csv)\n",
    "        df_test = df.iloc[int(-1 * dataset_sz*test_rate):]\n",
    "        df = df.iloc[:dataset_sz]\n",
    "    else: \n",
    "        df = gen_data_nonlinear(G, SIZE = dataset_sz)\n",
    "        df_test = gen_data_nonlinear(G, SIZE = int(dataset_sz*test_rate))\n",
    "# 做完数据统一标准化， mean =0, variance = 1    \n",
    "scaler = StandardScaler()\n",
    "if random_dag:\n",
    "    df = scaler.fit_transform(df)\n",
    "else:\n",
    "    if csv:\n",
    "        scaler.fit(pd.read_csv(csv))\n",
    "        df = scaler.transform(df)\n",
    "    else:\n",
    "        df = scaler.fit_transform(df)\n",
    "\n",
    "df_test = scaler.transform(df_test)\n",
    "\n",
    "X_test = df_test\n",
    "y_test = df_test[:,0]\n",
    "X_DAG = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b6522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset limits are 15.909454582195043 14.139267111691563 12.74255730592224\n",
      "fold =  1\n",
      "******* Doing dataset size =  1000 ****************\n",
      "Step 1, Loss= 10.0705  h_value: 0.0005245209\n",
      "Step 2, Loss= 2.0077  h_value: 0.0018644333\n",
      "Step 3, Loss= 1.1939  h_value: 0.0020570755\n",
      "Step 4, Loss= 0.9272  h_value: 0.0017166138\n",
      "Step 5, Loss= 0.8482  h_value: 0.0014219284\n",
      "Step 6, Loss= 0.8040  h_value: 0.0012245178\n",
      "Step 7, Loss= 0.7778  h_value: 0.0010480881\n",
      "Step 8, Loss= 0.7616  h_value: 0.00093364716\n",
      "Step 9, Loss= 0.7710  h_value: 0.0008544922\n",
      "Step 10, Loss= 0.7424  h_value: 0.0008172989\n",
      "Step 11, Loss= 0.7561  h_value: 0.0007791519\n",
      "Step 12, Loss= 0.7489  h_value: 0.00071430206\n",
      "Step 13, Loss= 0.7241  h_value: 0.00074768066\n",
      "Step 14, Loss= 0.7181  h_value: 0.0007543564\n",
      "Step 15, Loss= 0.7689  h_value: 0.0007200241\n",
      "Step 16, Loss= 0.7310  h_value: 0.00074863434\n",
      "Step 17, Loss= 0.7117  h_value: 0.0007429123\n",
      "Step 18, Loss= 0.7416  h_value: 0.00071048737\n",
      "Step 19, Loss= 0.7002  h_value: 0.0007209778\n",
      "Step 20, Loss= 0.7084  h_value: 0.0007343292\n",
      "Step 21, Loss= 0.7185  h_value: 0.00070858\n",
      "Step 22, Loss= 0.6978  h_value: 0.00074481964\n",
      "Step 23, Loss= 0.7087  h_value: 0.00073719025\n",
      "Step 24, Loss= 0.6935  h_value: 0.0007429123\n",
      "Step 25, Loss= 0.7172  h_value: 0.00071430206\n",
      "Step 26, Loss= 0.6914  h_value: 0.0007286072\n",
      "Step 27, Loss= 0.7035  h_value: 0.00072574615\n",
      "Step 28, Loss= 0.6903  h_value: 0.00070762634\n",
      "Step 29, Loss= 0.7702  h_value: 0.00074005127\n",
      "Step 30, Loss= 0.6989  h_value: 0.0007982254\n",
      "Step 31, Loss= 0.7073  h_value: 0.0008401871\n",
      "Step 32, Loss= 0.6910  h_value: 0.0008802414\n",
      "Step 33, Loss= 0.7134  h_value: 0.000875473\n",
      "Step 34, Loss= 0.7353  h_value: 0.000872612\n",
      "Step 35, Loss= 0.6785  h_value: 0.0009012222\n",
      "Step 36, Loss= 0.6933  h_value: 0.000872612\n",
      "Step 37, Loss= 0.6783  h_value: 0.0009126663\n",
      "Step 38, Loss= 0.6786  h_value: 0.0008831024\n",
      "Step 39, Loss= 0.6995  h_value: 0.00085926056\n",
      "Step 40, Loss= 0.7107  h_value: 0.00088214874\n",
      "Step 41, Loss= 0.6961  h_value: 0.0009441376\n",
      "Step 42, Loss= 0.6991  h_value: 0.0009765625\n",
      "Step 43, Loss= 0.6687  h_value: 0.00095939636\n",
      "Step 44, Loss= 0.6800  h_value: 0.0009994507\n",
      "Step 45, Loss= 0.6986  h_value: 0.0009813309\n",
      "Step 46, Loss= 0.6718  h_value: 0.00091552734\n",
      "Step 47, Loss= 0.6947  h_value: 0.00096035004\n",
      "Step 48, Loss= 0.6828  h_value: 0.000992775\n",
      "Step 49, Loss= 0.6668  h_value: 0.0009250641\n",
      "Step 50, Loss= 0.6864  h_value: 0.00096416473\n",
      "Saving model\n",
      "Step 51, Loss= 0.7296  h_value: 0.00092697144\n",
      "Step 52, Loss= 0.6959  h_value: 0.0010433197\n",
      "Saving model\n",
      "Step 53, Loss= 0.6983  h_value: 0.0009737015\n",
      "Saving model\n",
      "Step 54, Loss= 0.6938  h_value: 0.0010032654\n",
      "Saving model\n",
      "Step 55, Loss= 0.7082  h_value: 0.0010080338\n",
      "Step 56, Loss= 0.6881  h_value: 0.0010213852\n",
      "Step 57, Loss= 0.6723  h_value: 0.0010118484\n",
      "Step 58, Loss= 0.6699  h_value: 0.0010118484\n",
      "Step 59, Loss= 0.6908  h_value: 0.001074791\n",
      "Step 60, Loss= 0.6989  h_value: 0.0009832382\n",
      "Step 61, Loss= 0.6887  h_value: 0.000954628\n",
      "Saving model\n",
      "Step 62, Loss= 0.6871  h_value: 0.0011119843\n",
      "Saving model\n",
      "Step 63, Loss= 0.7149  h_value: 0.001074791\n",
      "Step 64, Loss= 0.6651  h_value: 0.0010700226\n",
      "Step 65, Loss= 0.6740  h_value: 0.0010499954\n",
      "Step 66, Loss= 0.6778  h_value: 0.0010881424\n",
      "Step 67, Loss= 0.6659  h_value: 0.0011224747\n",
      "Step 68, Loss= 0.6792  h_value: 0.0010814667\n",
      "Step 69, Loss= 0.6936  h_value: 0.0010890961\n",
      "Step 70, Loss= 0.6784  h_value: 0.0010681152\n",
      "Step 71, Loss= 0.6898  h_value: 0.0010948181\n",
      "Step 72, Loss= 0.6852  h_value: 0.0011663437\n",
      "Step 73, Loss= 0.6828  h_value: 0.0010814667\n",
      "Step 74, Loss= 0.6571  h_value: 0.0011253357\n",
      "Step 75, Loss= 0.6602  h_value: 0.0011425018\n",
      "Step 76, Loss= 0.6713  h_value: 0.0011301041\n",
      "Step 77, Loss= 0.6838  h_value: 0.0011520386\n",
      "Step 78, Loss= 0.6745  h_value: 0.0011749268\n",
      "Step 79, Loss= 0.6404  h_value: 0.0011634827\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = n_folds, random_state = 1,shuffle=True )\n",
    "\n",
    "fold = 0\n",
    "REG_castle = []\n",
    "print(\"Dataset limits are\", np.ptp(X_DAG), np.ptp(X_test), np.ptp(y_test))\n",
    "for train_idx, val_idx in kf.split(X_DAG):\n",
    "    fold += 1\n",
    "    print(\"fold = \", fold)\n",
    "    print(\"******* Doing dataset size = \", dataset_sz , \"****************\")\n",
    "    X_train = X_DAG[train_idx]\n",
    "    y_train = np.expand_dims(X_DAG[train_idx][:,0], -1)\n",
    "    X_val = X_DAG[val_idx]\n",
    "    y_val = X_DAG[val_idx][:,0]\n",
    "    castle = CASTLE(num_train = X_DAG.shape[0], num_inputs = X_DAG.shape[1], reg_lambda = reg_lambda, reg_beta = reg_beta,\n",
    "                        w_threshold = w_threshold, ckpt_file = ckpt_file)\n",
    "    num_nodes = np.shape(X_DAG)[1]\n",
    "    castle.fit(X_train, y_train, num_nodes, X_val, y_val, X_test, y_test)\n",
    "    W_est = castle.pred_W(X_DAG, np.expand_dims(X_DAG[:,0], -1))\n",
    "    print(W_est)\n",
    "\n",
    "    REG_castle.append(mean_squared_error(castle.pred(X_test), y_test))\n",
    "    print(\"MSE = \", mean_squared_error(castle.pred(X_test), y_test))\n",
    "\n",
    "    if fold > 1:\n",
    "        print(np.mean(REG_castle), np.std(REG_castle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdcb7a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 303\n",
      "MEAN = nan STD = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\envs\\mimic\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\software\\Anaconda3\\envs\\mimic\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "D:\\software\\Anaconda3\\envs\\mimic\\lib\\site-packages\\numpy\\core\\_methods.py:234: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "D:\\software\\Anaconda3\\envs\\mimic\\lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "D:\\software\\Anaconda3\\envs\\mimic\\lib\\site-packages\\numpy\\core\\_methods.py:226: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "with open(output_log, \"a\") as logfile:\n",
    "    if random_dag:\n",
    "        type_data = 'random nolinear data'\n",
    "        X_DAG.to_csv('evaluation data DAG.csv')\n",
    "        X_test.to_csv()\n",
    "    logfile.write('random'+ '\\n')\n",
    "    logfile.write(str(dataset_sz) + \",  \" +  format_str(np.mean(REG_castle), np.std(REG_castle)) + '\\n')\n",
    "print(\"MEAN =\", np.mean(REG_castle), \"STD =\", np.std(REG_castle)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0765c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DAG.to_csv('evaluation data DAG.csv')\n",
    "X_test.to_csv('evaluation data X.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
