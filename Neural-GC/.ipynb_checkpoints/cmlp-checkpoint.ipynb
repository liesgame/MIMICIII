{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d1e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from models.model_helper import activation_helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_series, lag, hidden, activation):\n",
    "        super(MLP, self).__init__()\n",
    "        self.activation = activation_helper(activation)\n",
    "\n",
    "        # Set up network.\n",
    "        layer = nn.Conv1d(num_series, hidden[0], lag)\n",
    "        modules = [layer]\n",
    "\n",
    "        for d_in, d_out in zip(hidden, hidden[1:] + [1]):\n",
    "            layer = nn.Conv1d(d_in, d_out, 1)\n",
    "            modules.append(layer)\n",
    "\n",
    "        # Register parameters.\n",
    "        self.layers = nn.ModuleList(modules)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.transpose(2, 1)\n",
    "        for i, fc in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                X = self.activation(X)\n",
    "            X = fc(X)\n",
    "\n",
    "        return X.transpose(2, 1)\n",
    "\n",
    "\n",
    "class cMLP(nn.Module):\n",
    "    def __init__(self, num_series, lag, hidden, activation='relu'):\n",
    "        '''\n",
    "        cMLP model with one MLP per time series.\n",
    "\n",
    "        Args:\n",
    "          num_series: dimensionality of multivariate time series.\n",
    "          lag: number of previous time points to use in prediction.\n",
    "          hidden: list of number of hidden units per layer.\n",
    "          activation: nonlinearity at each layer.\n",
    "        '''\n",
    "        super(cMLP, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.lag = lag\n",
    "        self.activation = activation_helper(activation)\n",
    "\n",
    "        # Set up networks.\n",
    "        self.networks = nn.ModuleList([\n",
    "            MLP(num_series, lag, hidden, activation)\n",
    "            for _ in range(num_series)])\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Perform forward pass.\n",
    "\n",
    "        Args:\n",
    "          X: torch tensor of shape (batch, T, p).\n",
    "        '''\n",
    "        return torch.cat([network(X) for network in self.networks], dim=2)\n",
    "\n",
    "    def GC(self, threshold=True, ignore_lag=True):\n",
    "        '''\n",
    "        Extract learned Granger causality.Extract learned Granger causality.\n",
    "\n",
    "        Args:\n",
    "          threshold: return norm of weights, or whether norm is nonzero.\n",
    "          ignore_lag: if true, calculate norm of weights jointly for all lags.\n",
    "\n",
    "        Returns:\n",
    "          GC: (p x p) or (p x p x lag) matrix. In first case, entry (i, j)\n",
    "            indicates whether variable j is Granger causal of variable i. In\n",
    "            second case, entry (i, j, k) indicates whether it's Granger causal\n",
    "            at lag k.\n",
    "        '''\n",
    "        if ignore_lag:\n",
    "            GC = [torch.norm(net.layers[0].weight, dim=(0, 2))\n",
    "                  for net in self.networks]\n",
    "        else:\n",
    "            GC = [torch.norm(net.layers[0].weight, dim=0)\n",
    "                  for net in self.networks]\n",
    "        GC = torch.stack(GC)\n",
    "        if threshold:\n",
    "            return (GC > 0).int()\n",
    "        else:\n",
    "            return GC\n",
    "\n",
    "\n",
    "class cMLPSparse(nn.Module):\n",
    "    def __init__(self, num_series, sparsity, lag, hidden, activation='relu'):\n",
    "        '''\n",
    "        cMLP model that only uses specified interactions.\n",
    "\n",
    "        Args:\n",
    "          num_series: dimensionality of multivariate time series.\n",
    "          sparsity: torch byte tensor indicating Granger causality, with size\n",
    "            (num_series, num_series).\n",
    "          lag: number of previous time points to use in prediction.\n",
    "          hidden: list of number of hidden units per layer.\n",
    "          activation: nonlinearity at each layer.\n",
    "        '''\n",
    "        super(cMLPSparse, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.lag = lag\n",
    "        self.activation = activation_helper(activation)\n",
    "        self.sparsity = sparsity\n",
    "\n",
    "        # Set up networks.\n",
    "        self.networks = []\n",
    "        for i in range(num_series):\n",
    "            num_inputs = int(torch.sum(sparsity[i].int()))\n",
    "            self.networks.append(MLP(num_inputs, lag, hidden, activation))\n",
    "\n",
    "        # Register parameters.\n",
    "        param_list = []\n",
    "        for i in range(num_series):\n",
    "            param_list += list(self.networks[i].parameters())\n",
    "        self.param_list = nn.ParameterList(param_list)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Perform forward pass.\n",
    "\n",
    "        Args:\n",
    "          X: torch tensor of shape (batch, T, p).\n",
    "        '''\n",
    "        return torch.cat([self.networks[i](X[:, :, self.sparsity[i]])\n",
    "                          for i in range(self.p)], dim=2)\n",
    "\n",
    "#  正则化 < lam * ir -> 0\n",
    "def prox_update(network, lam, lr, penalty):\n",
    "    '''\n",
    "    Perform in place proximal update on first layer weight matrix.\n",
    "\n",
    "    Args:\n",
    "      network: MLP network.\n",
    "      lam: regularization parameter.\n",
    "      lr: learning rate.\n",
    "      penalty: one of GL (group lasso), GSGL (group sparse group lasso),\n",
    "        H (hierarchical).\n",
    "    '''\n",
    "    W = network.layers[0].weight\n",
    "    hidden, p, lag = W.shape\n",
    "    if penalty == 'GL':\n",
    "        # 意味着对每一个属性的var 模型 延迟序列的权重， 100的\n",
    "        norm = torch.norm(W, dim=(0, 2), keepdim=True)\n",
    "        # norm 1 10 1\n",
    "        # W 100 10 5\n",
    "        W.data = ((W / torch.clamp(norm, min=(lr * lam)))\n",
    "                  * torch.clamp(norm - (lr * lam), min=0.0))\n",
    "        # 如果为0, 那就对一行为0\n",
    "        # 一行意味着一个属性\n",
    "    elif penalty == 'GSGL':\n",
    "        norm = torch.norm(W, dim=0, keepdim=True)\n",
    "        # norm = 1 10 5\n",
    "        W.data = ((W / torch.clamp(norm, min=(lr * lam)))\n",
    "                  * torch.clamp(norm - (lr * lam), min=0.0))\n",
    "        # norm = 1 10 1\n",
    "        norm = torch.norm(W, dim=(0, 2), keepdim=True)\n",
    "        W.data = ((W / torch.clamp(norm, min=(lr * lam)))\n",
    "                  * torch.clamp(norm - (lr * lam), min=0.0))\n",
    "    elif penalty == 'H':\n",
    "        # Lowest indices along third axis touch most lagged values.\n",
    "        for i in range(lag):\n",
    "            norm = torch.norm(W[:, :, :(i + 1)], dim=(0, 2), keepdim=True)\n",
    "            W.data[:, :, :(i+1)] = (\n",
    "                (W.data[:, :, :(i+1)] / torch.clamp(norm, min=(lr * lam)))\n",
    "                * torch.clamp(norm - (lr * lam), min=0.0))\n",
    "    else:\n",
    "        raise ValueError('unsupported penalty: %s' % penalty)\n",
    "\n",
    "\n",
    "def regularize(network, lam, penalty):\n",
    "    '''\n",
    "    Calculate regularization term for first layer weight matrix.\n",
    "\n",
    "    Args:\n",
    "      network: MLP network.\n",
    "      penalty: one of GL (group lasso), GSGL (group sparse group lasso),\n",
    "        H (hierarchical).\n",
    "    '''\n",
    "    W = network.layers[0].weight\n",
    "    hidden, p, lag = W.shape\n",
    "    if penalty == 'GL':\n",
    "        # 对每一行特征\n",
    "        return lam * torch.sum(torch.norm(W, dim=(0, 2)))\n",
    "    elif penalty == 'GSGL':\n",
    "        # 对每一行特征 + 特征和滞后一起\n",
    "        return lam * (torch.sum(torch.norm(W, dim=(0, 2)))\n",
    "                      + torch.sum(torch.norm(W, dim=0)))\n",
    "    elif penalty == 'H':\n",
    "        # Lowest indices along third axis touch most lagged values.\n",
    "        # 按滞后分层正则化\n",
    "        return lam * sum([torch.sum(torch.norm(W[:, :, :(i+1)], dim=(0, 2)))\n",
    "                          for i in range(lag)])\n",
    "    else:\n",
    "        raise ValueError('unsupported penalty: %s' % penalty)\n",
    "\n",
    "\n",
    "def ridge_regularize(network, lam):\n",
    "    '''Apply ridge penalty at all subsequent layers.'''\n",
    "    return lam * sum([torch.sum(fc.weight ** 2) for fc in network.layers[1:]])\n",
    "\n",
    "\n",
    "def restore_parameters(model, best_model):\n",
    "    '''Move parameter values from best_model to model.'''\n",
    "    for params, best_params in zip(model.parameters(), best_model.parameters()):\n",
    "        params.data = best_params\n",
    "\n",
    "\n",
    "def train_model_gista(cmlp, X, lam, lam_ridge, lr, penalty, max_iter,\n",
    "                      check_every=100, r=0.8, lr_min=1e-8, sigma=0.5,\n",
    "                      monotone=False, m=10, lr_decay=0.5,\n",
    "                      begin_line_search=True, switch_tol=1e-3, verbose=1):\n",
    "    '''\n",
    "    Train cMLP model with GISTA.\n",
    "\n",
    "    Args:\n",
    "      clstm: clstm model.\n",
    "      X: tensor of data, shape (batch, T, p).\n",
    "      lam: parameter for nonsmooth regularization.\n",
    "      lam_ridge: parameter for ridge regularization on output layer.\n",
    "      lr: learning rate.\n",
    "      penalty: type of nonsmooth regularization.\n",
    "      max_iter: max number of GISTA iterations.\n",
    "      check_every: how frequently to record loss.\n",
    "      r: for line search.\n",
    "      lr_min: for line search.\n",
    "      sigma: for line search.\n",
    "      monotone: for line search.\n",
    "      m: for line search.\n",
    "      lr_decay: for adjusting initial learning rate of line search.\n",
    "      begin_line_search: whether to begin with line search.\n",
    "      switch_tol: tolerance for switching to line search.\n",
    "      verbose: level of verbosity (0, 1, 2).\n",
    "    '''\n",
    "    p = cmlp.p\n",
    "    lag = cmlp.lag\n",
    "    cmlp_copy = deepcopy(cmlp)\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    lr_list = [lr for _ in range(p)]\n",
    "\n",
    "    # Calculate full loss.\n",
    "    mse_list = []\n",
    "    smooth_list = []\n",
    "    loss_list = []\n",
    "    for i in range(p):\n",
    "        net = cmlp.networks[i]\n",
    "        mse = loss_fn(net(X[:, :-1]), X[:, lag:, i:i+1])\n",
    "        ridge = ridge_regularize(net, lam_ridge)\n",
    "        smooth = mse + ridge\n",
    "        mse_list.append(mse)\n",
    "        smooth_list.append(smooth)\n",
    "        with torch.no_grad():\n",
    "            nonsmooth = regularize(net, lam, penalty)\n",
    "            loss = smooth + nonsmooth\n",
    "            loss_list.append(loss)\n",
    "\n",
    "    # Set up lists for loss and mse.\n",
    "    with torch.no_grad():\n",
    "        loss_mean = sum(loss_list) / p\n",
    "        mse_mean = sum(mse_list) / p\n",
    "    train_loss_list = [loss_mean]\n",
    "    train_mse_list = [mse_mean]\n",
    "\n",
    "    # For switching to line search.\n",
    "    line_search = begin_line_search\n",
    "\n",
    "    # For line search criterion.\n",
    "    done = [False for _ in range(p)]\n",
    "    assert 0 < sigma <= 1\n",
    "    assert m > 0\n",
    "    if not monotone:\n",
    "        last_losses = [[loss_list[i]] for i in range(p)]\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Backpropagate errors.\n",
    "        sum([smooth_list[i] for i in range(p) if not done[i]]).backward()\n",
    "\n",
    "        # For next iteration.\n",
    "        new_mse_list = []\n",
    "        new_smooth_list = []\n",
    "        new_loss_list = []\n",
    "\n",
    "        # Perform GISTA step for each network.\n",
    "        for i in range(p):\n",
    "            # Skip if network converged.\n",
    "            if done[i]:\n",
    "                new_mse_list.append(mse_list[i])\n",
    "                new_smooth_list.append(smooth_list[i])\n",
    "                new_loss_list.append(loss_list[i])\n",
    "                continue\n",
    "\n",
    "            # Prepare for line search.\n",
    "            step = False\n",
    "            lr_it = lr_list[i]\n",
    "            net = cmlp.networks[i]\n",
    "            net_copy = cmlp_copy.networks[i]\n",
    "\n",
    "            while not step:\n",
    "                # Perform tentative ISTA step.\n",
    "                for param, temp_param in zip(net.parameters(),\n",
    "                                             net_copy.parameters()):\n",
    "                    temp_param.data = param - lr_it * param.grad\n",
    "\n",
    "                # Proximal update.\n",
    "                prox_update(net_copy, lam, lr_it, penalty)\n",
    "                # param 更新好了\n",
    "\n",
    "                # Check line search criterion.\n",
    "                mse = loss_fn(net_copy(X[:, :-1]), X[:, lag:, i:i+1])\n",
    "                ridge = ridge_regularize(net_copy, lam_ridge)\n",
    "                smooth = mse + ridge\n",
    "                with torch.no_grad():\n",
    "                    nonsmooth = regularize(net_copy, lam, penalty)\n",
    "                    loss = smooth + nonsmooth\n",
    "                    # ? 可能是tolerance 阈值\n",
    "                    tol = (0.5 * sigma / lr_it) * sum(\n",
    "                        [torch.sum((param - temp_param) ** 2)\n",
    "                         for param, temp_param in\n",
    "                         zip(net.parameters(), net_copy.parameters())])\n",
    "                # max () last_losses 是二维数组， 行对应的属性\n",
    "                comp = loss_list[i] if monotone else max(last_losses[i])\n",
    "                \n",
    "                \n",
    "                # tolerance 容忍， 最大之前loss - 新的loss > tolerance \n",
    "                # 找到了lr ， 如果 loss 的 改变 显著时， 就 不用linear search\n",
    "                if not line_search or (comp - loss) > tol:\n",
    "                    step = True\n",
    "                    if verbose > 1:\n",
    "                        print('Taking step, network i = %d, lr = %f'\n",
    "                              % (i, lr_it))\n",
    "                        print('Gap = %f, tol = %f' % (comp - loss, tol))\n",
    "\n",
    "                    # For next iteration.\n",
    "                    new_mse_list.append(mse)\n",
    "                    new_smooth_list.append(smooth)\n",
    "                    new_loss_list.append(loss)\n",
    "\n",
    "                    # Adjust initial learning rate.\n",
    "                    # 如果 lr_list[i] = lr_it 那么 lr就完全不会改变\n",
    "                    lr_list[i] = (\n",
    "                        (lr_list[i] ** (1 - lr_decay)) * (lr_it ** lr_decay))\n",
    "\n",
    "                    if not monotone:\n",
    "                        if len(last_losses[i]) == m:\n",
    "                            last_losses[i].pop(0)\n",
    "                        last_losses[i].append(loss)\n",
    "                else:\n",
    "                    # Reduce learning rate.\n",
    "                    # 这里才是 linear search\n",
    "                    lr_it *= r\n",
    "                    if lr_it < lr_min:\n",
    "                        done[i] = True\n",
    "                        new_mse_list.append(mse_list[i])\n",
    "                        new_smooth_list.append(smooth_list[i])\n",
    "                        new_loss_list.append(loss_list[i])\n",
    "                        if verbose > 0:\n",
    "                            print('Network %d converged' % (i + 1))\n",
    "                        break\n",
    "\n",
    "            # Clean up.\n",
    "            net.zero_grad()\n",
    "\n",
    "            if step:\n",
    "                # Swap network parameters.\n",
    "                cmlp.networks[i], cmlp_copy.networks[i] = net_copy, net\n",
    "\n",
    "        # For next iteration.\n",
    "        mse_list = new_mse_list\n",
    "        smooth_list = new_smooth_list\n",
    "        loss_list = new_loss_list\n",
    "\n",
    "        # Check if all networks have converged.\n",
    "        if sum(done) == p:\n",
    "            if verbose > 0:\n",
    "                print('Done at iteration = %d' % (it + 1))\n",
    "            break\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            with torch.no_grad():\n",
    "                loss_mean = sum(loss_list) / p\n",
    "                mse_mean = sum(mse_list) / p\n",
    "                ridge_mean = (sum(smooth_list) - sum(mse_list)) / p\n",
    "                nonsmooth_mean = (sum(loss_list) - sum(smooth_list)) / p\n",
    "\n",
    "            train_loss_list.append(loss_mean)\n",
    "            train_mse_list.append(mse_mean)\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Total loss = %f' % loss_mean)\n",
    "                print('MSE = %f, Ridge = %f, Nonsmooth = %f'\n",
    "                      % (mse_mean, ridge_mean, nonsmooth_mean))\n",
    "                print('Variable usage = %.2f%%'\n",
    "                      % (100 * torch.mean(cmlp.GC().float())))\n",
    "\n",
    "            # Check whether loss has increased.\n",
    "            if not line_search:\n",
    "                if train_loss_list[-2] - train_loss_list[-1] < switch_tol:\n",
    "                    line_search = True\n",
    "                    if verbose > 0:\n",
    "                        print('Switching to line search')\n",
    "\n",
    "    return train_loss_list, train_mse_list\n",
    "\n",
    "\n",
    "def train_model_adam(cmlp, X, lr, max_iter, lam=0, lam_ridge=0, penalty='H',\n",
    "                     lookback=5, check_every=100, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    lag = cmlp.lag\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(cmlp.parameters(), lr=lr)\n",
    "    train_loss_list = []\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Calculate loss.\n",
    "        loss = sum([loss_fn(cmlp.networks[i](X[:, :-1]), X[:, lag:, i:i+1])\n",
    "                    for i in range(p)])\n",
    "\n",
    "        # Add penalty terms.\n",
    "        if lam > 0:\n",
    "            loss = loss + sum([regularize(net, lam, penalty)\n",
    "                               for net in cmlp.networks])\n",
    "        if lam_ridge > 0:\n",
    "            loss = loss + sum([ridge_regularize(net, lam_ridge)\n",
    "                               for net in cmlp.networks])\n",
    "\n",
    "        # Take gradient step.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cmlp.zero_grad()\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            mean_loss = loss / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(cmlp)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(cmlp, best_model)\n",
    "\n",
    "    return train_loss_list\n",
    "\n",
    "#迭代软阈值算法（ISTA）\n",
    "def train_model_ista(cmlp, X, lr, max_iter, lam=0, lam_ridge=0, penalty='H',\n",
    "                     lookback=5, check_every=100, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    lag = cmlp.lag\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    train_loss_list = []\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    # Calculate smooth error.\n",
    "    # 对每个属性进行预测， 比对\n",
    "    loss = sum([loss_fn(cmlp.networks[i](X[:, :-1]), X[:, lag:, i:i+1])\n",
    "                for i in range(p)])\n",
    "    # 对第二层及其之后的所有的weight 进行 L2, 对每个属性的分别处理， \n",
    "    ridge = sum([ridge_regularize(net, lam_ridge) for net in cmlp.networks])\n",
    "    # loss function\n",
    "    smooth = loss + ridge\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Take gradient step.\n",
    "        smooth.backward()\n",
    "        for param in cmlp.parameters():\n",
    "            param.data = param - lr * param.grad\n",
    "\n",
    "        # Take prox step.\n",
    "        if lam > 0:\n",
    "            for net in cmlp.networks:\n",
    "                prox_update(net, lam, lr, penalty)\n",
    "\n",
    "        cmlp.zero_grad()\n",
    "\n",
    "        # Calculate loss for next iteration.\n",
    "        loss = sum([loss_fn(cmlp.networks[i](X[:, :-1]), X[:, lag:, i:i+1])\n",
    "                    for i in range(p)])\n",
    "        ridge = sum([ridge_regularize(net, lam_ridge) for net in cmlp.networks])\n",
    "        smooth = loss + ridge\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            # Add nonsmooth penalty.\n",
    "            nonsmooth = sum([regularize(net, lam, penalty)\n",
    "                             for net in cmlp.networks])\n",
    "            mean_loss = (smooth + nonsmooth) / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "                print('Variable usage = %.2f%%'\n",
    "                      % (100 * torch.mean(cmlp.GC().float())))\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(cmlp)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(cmlp, best_model)\n",
    "\n",
    "    return train_loss_list\n",
    "\n",
    "\n",
    "def train_unregularized(cmlp, X, lr, max_iter, lookback=5, check_every=100,\n",
    "                        verbose=1):\n",
    "    '''Train model with Adam and no regularization.'''\n",
    "    lag = cmlp.lag\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(cmlp.parameters(), lr=lr)\n",
    "    train_loss_list = []\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Calculate loss.\n",
    "        pred = cmlp(X[:, :-1])\n",
    "        loss = sum([loss_fn(pred[:, :, i], X[:, lag:, i]) for i in range(p)])\n",
    "\n",
    "        # Take gradient step.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cmlp.zero_grad()\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            mean_loss = loss / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(cmlp)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(cmlp, best_model)\n",
    "\n",
    "    return train_loss_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch] *",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
