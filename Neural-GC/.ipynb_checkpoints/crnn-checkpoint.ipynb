{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, num_series, hidden, nonlinearity):\n",
    "        '''\n",
    "        RNN model with output layer to generate predictions.\n",
    "\n",
    "        Args:\n",
    "          num_series: number of input time series.\n",
    "          hidden: number of hidden units.\n",
    "        '''\n",
    "        super(RNN, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.hidden = hidden\n",
    "\n",
    "        # Set up network.\n",
    "        self.rnn = nn.RNN(num_series, hidden, nonlinearity=nonlinearity,\n",
    "                          batch_first=True)\n",
    "        self.rnn.flatten_parameters()\n",
    "        self.linear = nn.Conv1d(hidden, 1, 1)\n",
    "\n",
    "    def init_hidden(self, batch):\n",
    "        '''Initialize hidden states for RNN cell.'''\n",
    "        device = self.rnn.weight_ih_l0.device\n",
    "        return torch.zeros(1, batch, self.hidden, device=device)\n",
    "\n",
    "    def forward(self, X, hidden=None, truncation=None):\n",
    "        # Set up hidden state.\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(X.shape[0])\n",
    "\n",
    "        # Apply RNN.\n",
    "        # X 1 1000 100\n",
    "        # h 1 1 100\n",
    "        X, hidden = self.rnn(X, hidden)\n",
    "\n",
    "        # Calculate predictions using output layer.\n",
    "        X = X.transpose(2, 1)\n",
    "        # X 1 1 1000\n",
    "        X = self.linear(X)\n",
    "        # X 1000 1\n",
    "        return X.transpose(2, 1), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a50def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cRNN(nn.Module):\n",
    "    def __init__(self, num_series, hidden, nonlinearity='relu'):\n",
    "        '''\n",
    "        cRNN model with one RNN per time series.\n",
    "\n",
    "        Args:\n",
    "          num_series: dimensionality of multivariate time series.\n",
    "          hidden: number of units in RNN cell.\n",
    "          nonlinearity: nonlinearity of RNN cell.\n",
    "        '''\n",
    "        super(cRNN, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.hidden = hidden\n",
    "\n",
    "        # Set up networks.\n",
    "        # 一个属性一个\n",
    "        self.networks = nn.ModuleList([\n",
    "            RNN(num_series, hidden, nonlinearity) for _ in range(num_series)])\n",
    "\n",
    "    def forward(self, X, hidden=None):\n",
    "        '''\n",
    "        Perform forward pass.\n",
    "\n",
    "        Args:\n",
    "          X: torch tensor of shape (batch, T, p).\n",
    "          hidden: hidden states for RNN cell.\n",
    "        '''\n",
    "        if hidden is None:\n",
    "            hidden = [None for _ in range(self.p)]\n",
    "        pred = [self.networks[i](X, hidden[i])\n",
    "                for i in range(self.p)]\n",
    "        pred, hidden = zip(*pred)\n",
    "        pred = torch.cat(pred, dim=2)\n",
    "        return pred, hidden\n",
    "\n",
    "    def GC(self, threshold=True):\n",
    "        '''\n",
    "        Extract learned Granger causality.\n",
    "\n",
    "        Args:\n",
    "          threshold: return norm of weights, or whether norm is nonzero.\n",
    "\n",
    "        Returns:\n",
    "          GC: (p x p) matrix. Entry (i, j) indicates whether variable j is\n",
    "            Granger causal of variable i.\n",
    "        '''\n",
    "        GC = [torch.norm(net.rnn.weight_ih_l0, dim=0)\n",
    "              for net in self.networks]\n",
    "        GC = torch.stack(GC)\n",
    "        if threshold:\n",
    "            return (GC > 0).int()\n",
    "        else:\n",
    "            return GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6857f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cRNNSparse(nn.Module):\n",
    "    def __init__(self, num_series, sparsity, hidden, nonlinearity='relu'):\n",
    "        '''\n",
    "        cRNN model that only uses specified interactions.\n",
    "\n",
    "        Args:\n",
    "          num_series: dimensionality of multivariate time series.\n",
    "          sparsity: torch byte tensor indicating Granger causality, with size\n",
    "            (num_series, num_series).\n",
    "          hidden: number of units in RNN cell.\n",
    "          nonlinearity: nonlinearity of RNN cell.\n",
    "        '''\n",
    "        super(cRNNSparse, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.hidden = hidden\n",
    "        self.sparsity = sparsity\n",
    "\n",
    "        # Set up networks.\n",
    "        self.networks = nn.ModuleList([\n",
    "            RNN(int(torch.sum(sparsity[i].int())), hidden, nonlinearity)\n",
    "            for i in range(num_series)])\n",
    "\n",
    "    def forward(self, X, i=None, hidden=None, truncation=None):\n",
    "        '''Perform forward pass.\n",
    "\n",
    "        Args:\n",
    "          X: torch tensor of shape (batch, T, p).\n",
    "          i: index of the time series to forecast.\n",
    "          hidden: hidden states for RNN cell.\n",
    "        '''\n",
    "        if hidden is None:\n",
    "            hidden = [None for _ in range(self.p)]\n",
    "        pred = [self.networks[i](X[:, :, self.sparsity[i]], hidden[i])\n",
    "                for i in range(self.p)]\n",
    "        pred, hidden = zip(*pred)\n",
    "        pred = torch.cat(pred, dim=2)\n",
    "        # pred 1 , 1000, 10 (一个对应一个属性)\n",
    "        return pred, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803dbf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_update(network, lam, lr):\n",
    "    '''Perform in place proximal update on first layer weight matrix.'''\n",
    "    W = network.rnn.weight_ih_l0\n",
    "    norm = torch.norm(W, dim=0, keepdim=True)\n",
    "    W.data = ((W / torch.clamp(norm, min=(lam * lr)))\n",
    "              * torch.clamp(norm - (lr * lam), min=0.0))\n",
    "    network.rnn.flatten_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(network, lam):\n",
    "    '''Calculate regularization term for first layer weight matrix.'''\n",
    "    W = network.rnn.weight_ih_l0\n",
    "    return lam * torch.sum(torch.norm(W, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regularize(network, lam):\n",
    "    '''Apply ridge penalty at linear layer and hidden-hidden weights.'''\n",
    "    return lam * (\n",
    "        torch.sum(network.linear.weight ** 2) +\n",
    "        torch.sum(network.rnn.weight_hh_l0 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ec20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_parameters(model, best_model):\n",
    "    '''Move parameter values from best_model to model.'''\n",
    "    for params, best_params in zip(model.parameters(), best_model.parameters()):\n",
    "        params.data = best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_input(data, context):\n",
    "    '''\n",
    "    Arrange a single time series into overlapping short sequences.\n",
    "\n",
    "    Args:\n",
    "      data: time series of shape (T, dim).\n",
    "      context: length of short sequences.\n",
    "    '''\n",
    "    assert context >= 1 and isinstance(context, int)\n",
    "    input = torch.zeros(len(data) - context, context, data.shape[1],\n",
    "                        dtype=torch.float32, device=data.device)\n",
    "    target = torch.zeros(len(data) - context, context, data.shape[1],\n",
    "                         dtype=torch.float32, device=data.device)\n",
    "    for i in range(context):\n",
    "        start = i\n",
    "        end = len(data) - context + i\n",
    "        input[:, i, :] = data[start:end]\n",
    "        target[:, i, :] = data[start+1:end+1]\n",
    "    return input.detach(), target.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f0163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_gista(crnn, X, context, lam, lam_ridge, lr, max_iter,\n",
    "                      check_every=50, r=0.8, lr_min=1e-8, sigma=0.5,\n",
    "                      monotone=False, m=10, lr_decay=0.5,\n",
    "                      begin_line_search=True, switch_tol=1e-3, verbose=1):\n",
    "    '''\n",
    "    Train cRNN model with GISTA.\n",
    "\n",
    "    Args:\n",
    "      crnn: crnn model.\n",
    "      X: tensor of data, shape (batch, T, p).\n",
    "      context: length for short overlapping subsequences.\n",
    "      lam: parameter for nonsmooth regularization.\n",
    "      lam_ridge: parameter for ridge regularization on output layer.\n",
    "      lr: learning rate.\n",
    "      max_iter: max number of GISTA iterations.\n",
    "      check_every: how frequently to record loss.\n",
    "      r: for line search.\n",
    "      lr_min: for line search.\n",
    "      sigma: for line search.\n",
    "      monotone: for line search.\n",
    "      m: for line search.\n",
    "      lr_decay: for adjusting initial learning rate of line search.\n",
    "      begin_line_search: whether to begin with line search.\n",
    "      switch_tol: tolerance for switching to line search.\n",
    "      verbose: level of verbosity (0, 1, 2).\n",
    "    '''\n",
    "    p = crnn.p\n",
    "    crnn_copy = deepcopy(crnn)\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    lr_list = [lr for _ in range(p)]\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # Calculate full loss.\n",
    "    mse_list = []\n",
    "    smooth_list = []\n",
    "    loss_list = []\n",
    "    for i in range(p):\n",
    "        net = crnn.networks[i]\n",
    "        pred, _ = net(X)\n",
    "        mse = loss_fn(pred[:, :, 0], Y[:, :, i])\n",
    "        ridge = ridge_regularize(net, lam_ridge)\n",
    "        smooth = mse + ridge\n",
    "        mse_list.append(mse)\n",
    "        smooth_list.append(smooth)\n",
    "        with torch.no_grad():\n",
    "            nonsmooth = regularize(net, lam)\n",
    "            loss = smooth + nonsmooth\n",
    "            loss_list.append(loss)\n",
    "\n",
    "    # Set up lists for loss and mse.\n",
    "    with torch.no_grad():\n",
    "        loss_mean = sum(loss_list) / p\n",
    "        mse_mean = sum(mse_list) / p\n",
    "    train_loss_list = [loss_mean]\n",
    "    train_mse_list = [mse_mean]\n",
    "\n",
    "    # For switching to line search.\n",
    "    line_search = begin_line_search\n",
    "\n",
    "    # For line search criterion.\n",
    "    done = [False for _ in range(p)]\n",
    "    assert 0 < sigma <= 1\n",
    "    assert m > 0\n",
    "    if not monotone:\n",
    "        last_losses = [[loss_list[i]] for i in range(p)]\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Backpropagate errors.\n",
    "        sum([smooth_list[i] for i in range(p) if not done[i]]).backward()\n",
    "\n",
    "        # For next iteration.\n",
    "        new_mse_list = []\n",
    "        new_smooth_list = []\n",
    "        new_loss_list = []\n",
    "\n",
    "        # Perform GISTA step for each network.\n",
    "        for i in range(p):\n",
    "            # Skip if network converged.\n",
    "            if done[i]:\n",
    "                new_mse_list.append(mse_list[i])\n",
    "                new_smooth_list.append(smooth_list[i])\n",
    "                new_loss_list.append(loss_list[i])\n",
    "                continue\n",
    "\n",
    "            # Prepare for line search.\n",
    "            step = False\n",
    "            lr_it = lr_list[i]\n",
    "            net = crnn.networks[i]\n",
    "            net_copy = crnn_copy.networks[i]\n",
    "\n",
    "            while not step:\n",
    "                # Perform tentative ISTA step.\n",
    "                for param, temp_param in zip(net.parameters(),\n",
    "                                             net_copy.parameters()):\n",
    "                    temp_param.data = param - lr_it * param.grad\n",
    "\n",
    "                # Proximal update.\n",
    "                prox_update(net_copy, lam, lr_it)\n",
    "\n",
    "                # Check line search criterion.\n",
    "                pred, _ = net_copy(X)\n",
    "                mse = loss_fn(pred[:, :, 0], Y[:, :, i])\n",
    "                ridge = ridge_regularize(net_copy, lam_ridge)\n",
    "                smooth = mse + ridge\n",
    "                with torch.no_grad():\n",
    "                    nonsmooth = regularize(net_copy, lam)\n",
    "                    loss = smooth + nonsmooth\n",
    "                    tol = (0.5 * sigma / lr_it) * sum(\n",
    "                        [torch.sum((param - temp_param) ** 2)\n",
    "                         for param, temp_param in\n",
    "                         zip(net.parameters(), net_copy.parameters())])\n",
    "\n",
    "                comp = loss_list[i] if monotone else max(last_losses[i])\n",
    "                if not line_search or (comp - loss) > tol:\n",
    "                    step = True\n",
    "                    if verbose > 1:\n",
    "                        print('Taking step, network i = %d, lr = %f'\n",
    "                              % (i, lr_it))\n",
    "                        print('Gap = %f, tol = %f' % (comp - loss, tol))\n",
    "\n",
    "                    # For next iteration.\n",
    "                    new_mse_list.append(mse)\n",
    "                    new_smooth_list.append(smooth)\n",
    "                    new_loss_list.append(loss)\n",
    "\n",
    "                    # Adjust initial learning rate.\n",
    "                    lr_list[i] = (\n",
    "                        (lr_list[i] ** (1 - lr_decay)) * (lr_it ** lr_decay))\n",
    "\n",
    "                    if not monotone:\n",
    "                        if len(last_losses[i]) == m:\n",
    "                            last_losses[i].pop(0)\n",
    "                        last_losses[i].append(loss)\n",
    "                else:\n",
    "                    # Reduce learning rate.\n",
    "                    lr_it *= r\n",
    "                    if lr_it < lr_min:\n",
    "                        done[i] = True\n",
    "                        new_mse_list.append(mse_list[i])\n",
    "                        new_smooth_list.append(smooth_list[i])\n",
    "                        new_loss_list.append(loss_list[i])\n",
    "                        if verbose > 0:\n",
    "                            print('Network %d converged' % (i + 1))\n",
    "                        break\n",
    "\n",
    "            # Clean up.\n",
    "            net.zero_grad()\n",
    "\n",
    "            if step:\n",
    "                # Swap network parameters.\n",
    "                crnn.networks[i], crnn_copy.networks[i] = net_copy, net\n",
    "\n",
    "        # For next iteration.\n",
    "        mse_list = new_mse_list\n",
    "        smooth_list = new_smooth_list\n",
    "        loss_list = new_loss_list\n",
    "\n",
    "        # Check if all networks have converged.\n",
    "        if sum(done) == p:\n",
    "            if verbose > 0:\n",
    "                print('Done at iteration = %d' % (it + 1))\n",
    "            break\n",
    "\n",
    "        # Check progress\n",
    "        if (it + 1) % check_every == 0:\n",
    "            with torch.no_grad():\n",
    "                loss_mean = sum(loss_list) / p\n",
    "                mse_mean = sum(mse_list) / p\n",
    "                ridge_mean = (sum(smooth_list) - sum(mse_list)) / p\n",
    "                nonsmooth_mean = (sum(loss_list) - sum(smooth_list)) / p\n",
    "\n",
    "            train_loss_list.append(loss_mean)\n",
    "            train_mse_list.append(mse_mean)\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Total loss = %f' % loss_mean)\n",
    "                print('MSE = %f, Ridge = %f, Nonsmooth = %f'\n",
    "                      % (mse_mean, ridge_mean, nonsmooth_mean))\n",
    "                print('Variable usage = %.2f%%'\n",
    "                      % (100 * torch.mean(crnn.GC().float())))\n",
    "\n",
    "            # Check whether loss has increased.\n",
    "            if not line_search:\n",
    "                if train_loss_list[-2] - train_loss_list[-1] < switch_tol:\n",
    "                    line_search = True\n",
    "                    if verbose > 0:\n",
    "                        print('Switching to line search')\n",
    "\n",
    "    return train_loss_list, train_mse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40574499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_adam(crnn, X, context, lr, max_iter, lam=0, lam_ridge=0,\n",
    "                     lookback=5, check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(crnn.parameters(), lr=lr)\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Calculate loss.\n",
    "        pred = [crnn.networks[i](X)[0] for i in range(p)]\n",
    "        loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "\n",
    "        # Add penalty term.\n",
    "        if lam > 0:\n",
    "            loss = loss + sum([regularize(net, lam) for net in crnn.networks])\n",
    "\n",
    "        if lam_ridge > 0:\n",
    "            loss = loss + sum([ridge_regularize(net, lam_ridge)\n",
    "                               for net in crnn.networks])\n",
    "\n",
    "        # Take gradient step.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        crnn.zero_grad()\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            mean_loss = loss / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(crnn)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(crnn, best_model)\n",
    "\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ista(crnn, X, context, lr, max_iter, lam=0, lam_ridge=0,\n",
    "                     lookback=5, check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    # Calculate smooth error.\n",
    "    pred = [crnn.networks[i](X)[0] for i in range(p)]\n",
    "    loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "    ridge = sum([ridge_regularize(net, lam_ridge) for net in crnn.networks])\n",
    "    smooth = loss + ridge\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Take gradient step.\n",
    "        smooth.backward()\n",
    "        for param in crnn.parameters():\n",
    "            param.data -= lr * param.grad\n",
    "\n",
    "        # Take prox step.\n",
    "        if lam > 0:\n",
    "            for net in crnn.networks:\n",
    "                prox_update(net, lam, lr)\n",
    "\n",
    "        crnn.zero_grad()\n",
    "\n",
    "        # Calculate loss for next iteration.\n",
    "        pred = [crnn.networks[i](X)[0] for i in range(p)]\n",
    "        loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "        ridge = sum([ridge_regularize(net, lam_ridge)\n",
    "                     for net in crnn.networks])\n",
    "        smooth = loss + ridge\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            # Add nonsmooth penalty.\n",
    "            nonsmooth = sum([regularize(net, lam) for net in crnn.networks])\n",
    "            mean_loss = (smooth + nonsmooth) / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "                print('Variable usage = %.2f%%'\n",
    "                      % (100 * torch.mean(crnn.GC().float())))\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(crnn)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(crnn, best_model)\n",
    "\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unregularized(crnn, X, context, lr, max_iter, lookback=5,\n",
    "                        check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(crnn.parameters(), lr=lr)\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Calculate loss.\n",
    "        pred, hidden = crnn(X)\n",
    "        loss = sum([loss_fn(pred[:, :, i], Y[:, :, i]) for i in range(p)])\n",
    "\n",
    "        # Take gradient step.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        crnn.zero_grad()\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            mean_loss = loss / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(crnn)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(crnn, best_model)\n",
    "\n",
    "    return train_loss_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch] *",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
